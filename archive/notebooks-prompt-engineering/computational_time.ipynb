{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc88660-ea42-46d3-a837-904c62d8caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c11aa8-bed2-4db8-a930-cd9150cc6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874719ad-8294-44ad-bf18-999f9aa9ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = f\"\"\"\n",
    "Participant 1: \"Animal testing is necessary for medical research. It has contributed to numerous medical advancements and the \n",
    "development of life-saving treatments.\" \n",
    "Participant 2: \"While I understand the importance of medical research, \n",
    "we should also consider the ethical concerns associated with animal testing. Animals deserve to be treated with \n",
    "compassion and should not suffer for the sake of human benefits.\" \n",
    "Participant 3: \"There are alternatives to animal testing, such as in vitro testing and computer simulations, that can provide reliable results without causing harm \n",
    "to animals. We should prioritize the development and adoption of these alternatives.\" \n",
    "Participant 4: \"Animal testing should be regulated and minimized, but we cannot completely eliminate it at this stage. Striking a balance between scientific \n",
    "progress and animal welfare is crucial.\" \n",
    "Participant 5: \"We need more transparency and accountability in animal testing.\n",
    "Researchers should provide clear justifications for using animals and ensure that it is conducted in the most humane way \n",
    "possible.\n",
    "\n",
    "<br>Participant 1: \"The role of social media in political campaigns is a subject that has gained significant attention in recent \n",
    "years. It has become a powerful tool for politicians to engage with voters and spread their message. \n",
    "However, there are concerns about the spread of misinformation and the manipulation of public opinion through targeted ads.\" \n",
    "<br>Participant 2: \"I agree that social media provides a platform for political candidates to connect with a wider audience and \n",
    "mobilize support. However, the lack of regulation and transparency in political advertising on these platforms is a major \n",
    "issue that needs to be addressed.\" \n",
    "<br>Participant 3: \"I believe that social media has democratized political discourse and \n",
    "allowed marginalized voices to be heard. It provides a platform for grassroots movements and enables citizens to \n",
    "participate in political discussions like never before. We should focus on educating users about media literacy and critical \n",
    "thinking to combat misinformation.\" \n",
    "<br>Participant 4: \"While social media has its benefits, the algorithms used by these platforms tend to create echo chambers and \n",
    "reinforce existing biases. We need better regulation to ensure that diverse viewpoints are represented and to prevent the \n",
    "manipulation of public opinion through targeted content.\n",
    "\n",
    "<br>Speaker 1: \"We've seen a rise in youth violence in our community, and I'm worried. Maybe some sort of mentoring program \n",
    "could make a difference?\" \n",
    "<br>Speaker 2: \"Mentoring? Really? In my opinion, we need more police presence. That's the only language these kids understand.\" \n",
    "<br>Speaker 3: \"I wouldn't discount the value of a mentoring program so quickly. The problem isn't just law enforcement, but also \n",
    "social issues. Addressing those might help.\" \n",
    "<br>Speaker 4: \"If you ask me, this is a problem that begins at home. Many of these kids lack a good family structure. We need \n",
    "programs that support families too.\" \n",
    "<br>Speaker 5: \"All these are good points, but what about the role of schools? They're underfunded and struggling to offer good \n",
    "education and extracurricular activities.\" \n",
    "<br>Speaker 6: \"Yes, and we also have to remember the role of peer pressure in youth violence. We need programs that teach our \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020b65c2-c781-43c2-9a79-b2397bc8acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_2 = f\"\"\"\n",
    "Participant 1: \"Animal testing is necessary for medical research. It has contributed to numerous medical advancements and the \n",
    "development of life-saving treatments.\" \n",
    "Participant 2: \"While I understand the importance of medical research, \n",
    "we should also consider the ethical concerns associated with animal testing. Animals deserve to be treated with \n",
    "compassion and should not suffer for the sake of human benefits.\" \n",
    "Participant 3: \"There are alternatives to animal testing, such as in vitro testing and computer simulations, that can provide reliable results without causing harm \n",
    "to animals. We should prioritize the development and adoption of these alternatives.\" \n",
    "Participant 4: \"Animal testing should be regulated and minimized, but we cannot completely eliminate it at this stage. Striking a balance between scientific \n",
    "progress and animal welfare is crucial.\" \n",
    "Participant 5: \"We need more transparency and accountability in animal testing.\n",
    "Researchers should provide clear justifications for using animals and ensure that it is conducted in the most humane way \n",
    "possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911324bf-ba69-4103-a360-471dc753f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_3 = f\"\"\"\n",
    "<br>Speaker 1: \"We've seen a rise in youth violence in our community, and I'm worried. Maybe some sort of mentoring program \n",
    "could make a difference?\" \n",
    "<br>Speaker 2: \"Mentoring? Really? In my opinion, we need more police presence. That's the only language these kids understand.\" \n",
    "<br>Speaker 3: \"I wouldn't discount the value of a mentoring program so quickly. The problem isn't just law enforcement, but also \n",
    "social issues. Addressing those might help.\" \n",
    "<br>Speaker 4: \"If you ask me, this is a problem that begins at home. Many of these kids lack a good family structure. We need \n",
    "programs that support families too.\" \n",
    "<br>Speaker 5: \"All these are good points, but what about the role of schools? They're underfunded and struggling to offer good \n",
    "education and extracurricular activities.\" \n",
    "<br>Speaker 6: \"Yes, and we also have to remember the role of peer pressure in youth violence. We need programs that teach our \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219d48ab-e1ee-4f70-b2b7-ba9b356d5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_4 = f\"\"\"\n",
    "P1: \"Have you all been following the news about the rising sea levels? Our coastal cities are under threat and we can't just \n",
    "sit and watch. We need to be proactive in our mitigation efforts, perhaps even consider relocating our operations to a safer, \n",
    "inland location.\" P2: \"I share your concerns, P1, but relocating seems a bit extreme, doesn't it? It's a costly measure. I \n",
    "think we should focus on increasing our preparedness for extreme weather events. Let's reinforce our infrastructure, have \n",
    "contingency plans in place. Better safe than sorry, right?\" P3: \"While I appreciate both points, I must stress that we can't \n",
    "tackle this issue single-handedly. We need to lobby for more robust government measures. It's a national issue.\" \n",
    "P4: \"Indeed, P3. But we also have a responsibility to reduce our own carbon footprint. Maybe, investing in green \n",
    "technologies or shifting to cleaner production methods?\" P5: \"I like that idea, P4. In fact, it might even prove to be \n",
    "beneficial for our brand image. We can portray ourselves as a responsible, environmentally-conscious corporation.\" \n",
    "P6: \"All points considered, we need to come up with a comprehensive plan. Let's get our teams on this - risk management, \n",
    "PR, everyone. And let's engage with local communities, they need to be part of this conversation too.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfafc6-03c9-4c61-a1c5-fe29578b2e86",
   "metadata": {},
   "source": [
    "#### Helper function to get completions from model based on a list of prompts; model to be called can be changed (e.g., GPT-3.5-turbo instead of GPT-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a04dea-b453-4119-9372-cbfc0376fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd82f13-631f-4cb4-a309-6c1758c53d9e",
   "metadata": {},
   "source": [
    "#### Function for getting completions from OpenAI instructing model to identify topics based on a discussion transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d91b09f-7db5-4a63-a271-bf02c8646dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(transcript):\n",
    "    prompt_topic = f\"\"\"\n",
    "    You are an assistant for group discussions, specializing in keeping track of and documenting the discussion,\n",
    "that is, the topics discussed, the viewpoints/positions on each topic, and the arguments/explanations given in support of each viewpoint.\n",
    "\n",
    "Identify the main topic discussed in the discussion transcript, delimited with triple backticks.\n",
    "\n",
    "Check the following steps but don't print each step, only print what is asked:\n",
    "\n",
    "Step 1: Identify the main topic discussed in the discussion transcript. Take the time to read the complete transcript and \n",
    "        understand it, don't take words as a topic, analyze the arguments in the transcript and then decide on the topic. \\\n",
    "Step 2: Analyze each topic (if there is more than one) and merge those into one main topic.\n",
    "        If there are topics that are as important as the main topic, display them as secondary topics, but only if they are \n",
    "        closely linked to the main topic of discussion in the transcript. \\\n",
    "Step 3: State the necessary main topic of the entire transcript in a concise, descriptive sentence,\n",
    "        in up to 3 words for each topic. Consider the frequency of mentions, relevance, and significance of each topic.\n",
    "        Discard any topic that is not at least 50% as relevant as the main topic. \\\n",
    "Step 4: Review the main topic and the transcript again. If any secondary topics are not closely related to the main topic,\n",
    "        discard them and print only the main topic. \\\n",
    "Step 5: Provide the output in a JSON format where the key is each topic and the values are the text 'viewpoints'. Don't write\n",
    "the text 'main topic' and don't display secondary topics.\n",
    "        for example:\n",
    "            {{main topic: 'viewpoint'}}\n",
    "\n",
    "    Review transcript: '''{transcript}'''\n",
    "    \"\"\"\n",
    "\n",
    "    topics = get_completion(prompt_topic)\n",
    "    topics_json = json.loads(topics)\n",
    "\n",
    "    return topics_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6527777a-f20b-4ca5-9a39-145bcead4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(transcript):\n",
    "    start_time = time.time()  # Start measuring time\n",
    "    topics_test = get_topics(transcript)\n",
    "    end_time = time.time()  # Stop measuring time\n",
    "    execution_time = end_time - start_time  # Calculate the execution time\n",
    "    return {\"topics\": topics_test, \"execution_time\": execution_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9de20fdc-1955-471e-a9c3-e1d86a9ec745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topics': {'Rising Sea Levels': 'viewpoint'}, 'execution_time': 2.6916348934173584}\n"
     ]
    }
   ],
   "source": [
    "topics = process_transcript(transcript_4)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98e738-727a-4418-ad68-edfc6819cab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7839ddc-fb5c-49b0-8165-c4da6e29217a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4e025-c7d0-45fb-adbb-f196d07e2935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f21bab39-8204-40f5-9843-491d213a287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viewpoints_by_topic(topics,transcript):\n",
    "    result = {}\n",
    "    for topic, excerpt in topics.items():\n",
    "        prompt = f\"\"\"\n",
    "        You are an assistant for group discussions, specialized on keeping track and documenting the discussion,\n",
    "        that is, the topics discussed, the viewpoints/positions on each topic and the arguments/explanations given in support of each viewpoint.\n",
    "        For the topic '{topic}'elimited by triple hashtags, analyse the corresponding excerpt from the discussion.\n",
    "        Your task is to identify all the viewpoints expressed on the topic.\n",
    "\n",
    "        Proceed according to the following steps:\n",
    "\n",
    "        Step 1: In the below discussion transcript, delimited by triple backticks,\n",
    "                locate the excerpt corresponding to the topic and consider the rest\n",
    "                of the transcript as context for the subsequent steps.\n",
    "        Step 2: Are there one or several viewpoints being expressed in the excerpt?\n",
    "                A \"viewpoint\" refers to \"one's perspective of opinion on a particular topic\".\n",
    "        Step 3: If there is only one viewpoint, summarize the viewpoint in 3 keywords max,\n",
    "                more keywords only if necessary to fully grasp the viewpoint. Viewpoint keywords\n",
    "                should be expressed as noun phrases that describe the viewpoint in a depersonalized manner.\n",
    "                For example, instead of “Supports Renewables”, the viewpoint keyword should be “Support for Renewables”.\n",
    "                Instead of “Believes in Traditional Energy”, the viewpoint keyword should be “Belief in Traditional Energy”.\n",
    "\n",
    "                If there are several viewpoints, summarize each viewpoint in 3 keywords max, more keywords only if\n",
    "                necessary to fully grasp the topic. Viewpoint keywords should be expressed as noun phrases that describe\n",
    "                the viewpoint in a depersonalized manner, as explained in the instruction for one viewpoint.\n",
    "        Step 4: Disregard viewpoints that are not relevant to the current topic or more relevant to another topic. Only if a viewpoint is equally relevant to multiple topics, include it under all relevant topics.\n",
    "        Step 5: Identify any linkages between viewpoints that build upon each other or propose solutions to identified issues. For instance, if a viewpoint such as 'Media literacy and critical thinking' is expressed as a solution to the issue identified in another viewpoint like 'Concerns about misinformation', classify it as a sub-viewpoint of the latter. Represent these sub-viewpoints appropriately within the hierarchical structure of the result dictionary.\n",
    "        Step 6: Identify viewpoints that convey essentially the same stance on the topic. For example, viewpoints like 'Lack of regulation and transparency' and 'Need for better regulation' express similar concerns regarding the need for increased regulation in the domain. In such cases, merge these viewpoints into a single unified viewpoint that encapsulates both perspectives. Ensure this is reflected in the summary of viewpoints in the result dictionary.\n",
    "        Step 7: Append the topics with the identified viewpoints as sub-keys. Format the dictionary in JSON-format.\n",
    "\n",
    "        Only include the appended dictionary in your response.\n",
    "\n",
    "        ```{transcript}```\n",
    "        \"\"\"\n",
    "        viewpoints = get_completion(prompt)\n",
    "        viewpoints_json = json.loads(viewpoints)\n",
    "        result.update(viewpoints_json)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0908473c-21ef-4188-9dc3-f91ff82707fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rising Sea Levels': {'Relocating Operations': {}, 'Increasing Preparedness': {}, 'Lobbying for Government Measures': {}, 'Reducing Carbon Footprint': {'Investing in Green Technologies': {}, 'Cleaner Production Methods': {}}, 'Improving Brand Image': {}, 'Comprehensive Plan': {'Risk Management': {}, 'Public Relations': {}, 'Community Engagement': {}}}, 'execution_time': {'Relocation as mitigation': {}, 'Preparedness for extreme weather': {}, 'Lobbying for government measures': {}, 'Reducing carbon footprint': {}, 'Brand image benefits': {}, 'Comprehensive plan': {'Involving multiple teams': {}, 'Engaging local communities': {}}}}\n",
      "Execution Time: 33.50298309326172 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result_view = get_viewpoints_by_topic(topics, transcript_4)\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "# Print the result and execution time\n",
    "print(result_view)\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a8920-4228-484b-af2c-66b9da5ac58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e238d-ede3-48c8-ad58-b49c0a8016f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c65139-b4c8-4fce-b40c-bfb446cb254b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181fbd1-8638-49e5-a126-dbbc98d67fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64328df5-2539-4765-8e2d-c102c1bc7d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8539a73-e6e6-495d-aa0f-71729e6dd791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d06c4d9-017e-4604-af0a-0c1e63d95cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments_by_viewpoint(viewpoints_by_topic):\n",
    "    result = {}\n",
    "    for topic, viewpoints in viewpoints_by_topic.items():\n",
    "        arguments = {}\n",
    "        for viewpoint, summary in viewpoints.items():\n",
    "            prompt = f\"\"\"\n",
    "            You are an assistant for group discussions, specialized in keeping track and documenting the discussion,\n",
    "            that is, the topics discussed, the viewpoints/positions on each topic, and the arguments/explanations given in support of each viewpoint.\n",
    "            For the topic '{topic}' and viewpoint '{viewpoint}', you need to provide the arguments/explanations\n",
    "            given in support of this viewpoint.\n",
    "\n",
    "            Proceed according to the following steps:\n",
    "\n",
    "            Step 1: Review the transcript excerpt related to the viewpoint '{viewpoint}' on the topic '{topic}'.\n",
    "            Step 2: Identify the arguments or explanations provided in support of this viewpoint.\n",
    "            Step 3: Summarize the arguments/explanations in one to two sentences.\n",
    "\n",
    "            Excerpt:\n",
    "            {summary}\n",
    "            \"\"\"\n",
    "\n",
    "            arguments_completion = get_completion(prompt)\n",
    "            arguments_json = json.loads(arguments_completion)\n",
    "            arguments[viewpoint] = arguments_json\n",
    "\n",
    "        result[topic] = arguments\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25a046-3426-4f06-b4ca-72ee8175dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(transcript):\n",
    "    topics = get_topics(transcript)\n",
    "    viewpoints = get_viewpoints_by_topic(topics, transcript)\n",
    "    arguments = get_arguments_by_viewpoint(viewpoints)\n",
    "    return {\"topics\": topics, \"viewpoints\": viewpoints, \"arguments\": arguments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e79e175-0442-41e4-8db5-6cf456fc6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript_batch(transcripts):\n",
    "    results = []\n",
    "    for transcript in transcripts:\n",
    "        result = process_transcript(transcript)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88774b80-ec59-4477-8b35-c5738e2596ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fa7e56834b8a406262a344005a49f257 in your message.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mprocess_transcript\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_transcript\u001b[39m(transcript):\n\u001b[0;32m----> 2\u001b[0m     topics \u001b[38;5;241m=\u001b[39m \u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     viewpoints \u001b[38;5;241m=\u001b[39m get_viewpoints_by_topic(topics, transcript)\n\u001b[1;32m      4\u001b[0m     arguments \u001b[38;5;241m=\u001b[39m get_arguments_by_viewpoint(viewpoints)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mget_topics\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topics\u001b[39m(transcript):\n\u001b[1;32m      2\u001b[0m     prompt_topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    You are an assistant for group discussions, specializing in keeping track of and documenting the discussion,\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    that is, the topics discussed, the viewpoints/positions on each topic, and the arguments/explanations given in support of each viewpoint.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    Review transcript: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtranscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m     topics \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_topic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     topics_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(topics)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m topics_json\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion\u001b[39m(prompt, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/deliberAIde/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/deliberAIde/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/deliberAIde/lib/python3.10/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/deliberAIde/lib/python3.10/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/deliberAIde/lib/python3.10/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fa7e56834b8a406262a344005a49f257 in your message.)"
     ]
    }
   ],
   "source": [
    "result = process_transcript(transcript)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ac733-be2f-49e3-94e7-7215b652e2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
